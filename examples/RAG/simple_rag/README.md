<!--
SPDX-FileCopyrightText: Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
SPDX-License-Identifier: Apache-2.0

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->


# Simple RAG Example
This is a simple example RAG application to showcase how one can configure and use the  Retriever component. This example includes:
 - The config file to run the workflow
 - A docker compose deployment for standing up Milvus
 - A script for scraping data from URLs and storing it in Milvus

 This example is intended to be illustrative and demonstrate how someone could build a simple RAG application using the retriever component and use it with an agent without any additional code required!

## Key Features

- **Milvus Vector Database Integration:** Demonstrates the `milvus_retriever` component for storing and retrieving document embeddings from CUDA and MCP documentation.
- **ReAct Agent with RAG:** Shows how a `react_agent` can use retriever tools to answer questions by searching through indexed documentation.
- **Long-term Memory with Mem0:** Includes integration with Mem0 platform for persistent memory, allowing the agent to remember user preferences across sessions.
- **Multi-Collection Retrieval:** Demonstrates multiple retriever tools (`cuda_retriever_tool` and `mcp_retriever_tool`) for searching different knowledge bases.
- **Additional Tool Integration:** Shows how to extend the RAG system with complementary tools like `tavily_internet_search` and `code_generation` for comprehensive question answering.

## Quickstart: RAG with Milvus

### Installation and Setup
If you have not already done so, follow the instructions in the [Install Guide](../../../docs/source/quick-start/installing.md#install-from-source) to create the development environment and install AIQ toolkit, and follow the [Obtaining API Keys](../../../docs/source/quick-start/installing.md#obtaining-api-keys) instructions to obtain an NVIDIA API key.

1. From the root directory of the AIQ toolkit library, run the following commands:
    ```bash
    uv pip install -e examples/RAG/simple_rag
    ```

1. Start the docker compose [Skip this step if you already have Milvus running]
    ```bash
    docker compose -f examples/RAG/simple_rag/deploy/docker-compose.yaml up -d
    ```
    > Note: It can take some time for Milvus to start up. You can check the logs with:
    ```bash
    docker compose -f examples/RAG/simple_rag/deploy/docker-compose.yaml logs --follow
    ```
1. In a new terminal, from the root of the AIQ toolkit repository, run the provided bash script to store the data in a Milvus collection. By default the script will scrape a few pages from the CUDA documentation and store the data in a Milvus collection called `cuda_docs`. It will also pull a few pages of information about the Model Context Protocol (MCP) and store it in a collection called `mcp_docs`.

    Export your NVIDIA API key:
    ```bash
    export NVIDIA_API_KEY=<YOUR API KEY HERE>
    ```

    ```bash
    source .venv/bin/activate
    scripts/bootstrap_milvus.sh
    ```

    If Milvus is running the script should work out of the box. If you want to customize the script the arguments are shown below.
    ```bash
    python scripts/langchain_web_ingest.py --help
    ```
    ```console
    usage: langchain_web_ingest.py [-h] [--urls URLS] [--collection_name COLLECTION_NAME] [--milvus_uri MILVUS_URI] [--clean_cache]

    options:
    -h, --help            show this help message and exit
    --urls URLS           Urls to scrape for RAG context (default: ['https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html', 'https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html', 'https://docs.nvidia.com/cuda/cuda-c-
                            best-practices-guide/index.html', 'https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html'])
    --collection_name COLLECTION_NAME, -n COLLECTION_NAME
                            Collection name for the data. (default: cuda_docs)
    --milvus_uri MILVUS_URI, -u MILVUS_URI
                            Milvus host URI (default: http://localhost:19530)
    --clean_cache         If true, deletes local files (default: False)
    ```

1. Configure your Agent to use the Milvus collections for RAG. We have pre-configured a configuration file for you in `examples/RAG/simple_rag/configs/milvus_rag_config.yml`. You can modify this file to point to your Milvus instance and collections or add tools to your agent. The agent, by default, is a `tool_calling` agent that can be used to interact with the retriever component. The configuration file is shown below. You can also modify your agent to be another one of the AIQ toolkit pre-built agent implementations such as the `react_agent`

    ```yaml
    general:
      use_uvloop: true

    retrievers:
      cuda_retriever:
        _type: milvus_retriever
        uri: http://localhost:19530
        collection_name: "cuda_docs"
        embedding_model: milvus_embedder
        top_k: 10
      mcp_retriever:
        _type: milvus_retriever
        uri: http://localhost:19530
        collection_name: "mcp_docs"
        embedding_model: milvus_embedder
        top_k: 10

    functions:
      cuda_retriever_tool:
        _type: aiq_retriever
        retriever: cuda_retriever
        topic: Retrieve documentation for NVIDIA's CUDA library
      mcp_retriever_tool:
        _type: aiq_retriever
        retriever: mcp_retriever
        topic: Retrieve information about Model Context Protocol (MCP)

    llms:
      nim_llm:
        _type: nim
        model_name: meta/llama-3.3-70b-instruct
        temperature: 0
        max_tokens: 4096
        top_p: 1

    embedders:
      milvus_embedder:
        _type: nim
        model_name: nvidia/nv-embedqa-e5-v5
        truncate: "END"

    workflow:
      _type: react_agent
      tool_names:
       - cuda_retriever_tool
         - mcp_retriever_tool
      verbose: true
      llm_name: nim_llm
    ```

    If you have a different Milvus instance or collection names, you can modify the `retrievers` section of the config file to point to your instance and collections. You can also add additional functions as tools for your agent in the `functions` section.

1. Run the workflow
    ```bash
    aiq run --config_file examples/RAG/simple_rag/configs/milvus_rag_config.yml --input "How do I install CUDA"
    ```
   The expected output of running the above command is:
    ```console
    $ aiq run --config_file examples/RAG/simple_rag/configs/milvus_rag_config.yml --input "How do I install CUDA"
    2025-04-23 16:45:01,698 - aiq.runtime.loader - WARNING - Loading module 'aiq_automated_description_generation.register' from entry point 'aiq_automated_description_generation' took a long time (469.127893 ms). Ensure all imports are inside your registered functions.
    2025-04-23 16:45:02,024 - aiq.cli.commands.start - INFO - Starting AIQ toolkit from config file: 'examples/RAG/simple_rag/configs/milvus_rag_config.yml'
    2025-04-23 16:45:02,032 - aiq.cli.commands.start - WARNING - The front end type in the config file (fastapi) does not match the command name (console). Overwriting the config file front end.
    2025-04-23 16:45:02,169 - aiq.retriever.milvus.retriever - INFO - Mivlus Retriever using _search for search.
    2025-04-23 16:45:02,177 - aiq.retriever.milvus.retriever - INFO - Mivlus Retriever using _search for search.

    Configuration Summary:
    --------------------
    Workflow Type: react_agent
    Number of Functions: 2
    Number of LLMs: 1
    Number of Embedders: 1
    Number of Memory: 0
    Number of Retrievers: 2

    2025-04-23 16:45:03,203 - aiq.agent.react_agent.agent - INFO -
    ------------------------------
    [AGENT]
    Agent input: How do I install CUDA
    Agent's thoughts:
    Thought: To answer the user's question, I need to find information about installing CUDA.
    Action: cuda_retriever_tool
    Action Input: {"query": "install CUDA"}

    ------------------------------
    2025-04-23 16:45:03,511 - aiq.tool.retriever - INFO - Retrieved 10 records for query install CUDA.
    2025-04-23 16:45:03,513 - aiq.agent.react_agent.agent - INFO -
    ------------------------------
    [AGENT]
    Calling tools: cuda_retriever_tool
    Tool's input: {"query": "install CUDA"}
    Tool's response:
    {"results": [{"page_content": "1. Introduction \u2014 Installation Guide Windows 12.8 documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n1.1. System Requirements\n1.2. About This Document\n\n\n2. Installing CUDA Development Tools\n2.1. Verify You Have a CUDA-capable GPU\n2.2. Download the NVIDIA CUDA Toolkit\n2.3. Install the CUDA Software\n2.3.1. Uninstalling the CUDA Software\n\n\n2.4. Using Conda to Install the CUDA Software\n2.4.1. Conda Overview\n2.4.2. Installation\n2.4.3. Uninstallation\n2.4.4. Installing Previous CUDA Releases\n\n\n2.5. Use a Suitable Driver Model\n2.6. Verify the Installation\n2.6.1. Running the Compiled Examples\n\n\n\n\n3. Pip Wheels\n4. Compiling CUDA Programs\n4.1. Compiling Sample Projects\n4.2. Sample Projects\n4.3. Build Customizations for New Projects\n4.4. Build Customizations for Existing Projects\n\n\n5. Additional Considerations\n6. Notices\n6.1. Notice\n6.2. OpenCL\n6.3. Trademarks\n\n\n...
    ------------------------------
    2025-04-23 16:45:06,407 - aiq.agent.react_agent.agent - INFO -
    ------------------------------
    [AGENT]
    Agent input: How do I install CUDA
    Agent's thoughts:
    Thought: The provided tool output contains detailed instructions for installing CUDA on various Linux distributions and Windows. To answer the user's question, I will summarize the general steps for installing CUDA.

    Final Answer: To install CUDA, you need to follow these general steps:

    1. Verify that your system has a CUDA-capable GPU.
    2. Choose an installation method: local repo or network repo.
    3. Download the NVIDIA CUDA Toolkit from the official NVIDIA website.
    4. Install the CUDA Toolkit using the chosen installation method.
    5. Perform post-installation actions, such as updating the Apt repository cache and installing additional packages.

    For specific instructions, please refer to the official NVIDIA documentation for your operating system: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html or https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html.
    ------------------------------
    2025-04-23 16:45:06,412 - aiq.front_ends.console.console_front_end_plugin - INFO -
    --------------------------------------------------
    Workflow Result:
    ['To install CUDA, you need to follow these general steps:\n\n1. Verify that your system has a CUDA-capable GPU.\n2. Choose an installation method: local repo or network repo.\n3. Download the NVIDIA CUDA Toolkit from the official NVIDIA website.\n4. Install the CUDA Toolkit using the chosen installation method.\n5. Perform post-installation actions, such as updating the Apt repository cache and installing additional packages.\n\nFor specific instructions, please refer to the official NVIDIA documentation for your operating system: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html or https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html.']
    --------------------------------------------------
    ```

## Adding Long-Term Agent Memory
If you want to add long-term memory to your agent, you can do so by adding a `memory` section to your configuration file. The memory section is used to store information that the agent can use to provide more contextually relevant answers to the user's questions. The memory section can be used to store information such as user preferences, past interactions, or any other information that the agent needs to remember.

### Prerequisites
This section requires an API key for integration with the Mem0 Platform. To create an API key, refer to the instructions in the [Mem0 Platform Guide](https://docs.mem0.ai/platform/quickstart). Once you have created your API key, export it as an environment variable:
```bash
export MEM0_API_KEY=<MEM0 API KEY HERE>
```

### Adding Memory to the Agent
Adding the ability to add and retrieve long-term memory to the agent is just a matter of adding a `memory` section to the configuration file. The AIQ toolkit built-in abstractions for long term memory management allow agents to automatically interact with them as tools. We will use the following configuration file, which you can also find in the `configs` directory.

```yaml
general:
  use_uvloop: true

memory:
  saas_memory:
    _type: mem0_memory

retrievers:
  cuda_retriever:
    _type: milvus_retriever
    uri: http://localhost:19530
    collection_name: "cuda_docs"
    embedding_model: milvus_embedder
    top_k: 10
  mcp_retriever:
    _type: milvus_retriever
    uri: http://localhost:19530
    collection_name: "mcp_docs"
    embedding_model: milvus_embedder
    top_k: 10

functions:
  cuda_retriever_tool:
    _type: aiq_retriever
    retriever: cuda_retriever
    topic: Retrieve documentation for NVIDIA's CUDA library
  mcp_retriever_tool:
    _type: aiq_retriever
    retriever: mcp_retriever
    topic: Retrieve information about Model Context Protocol (MCP)
  add_memory:
    _type: add_memory
    memory: saas_memory
    description: |
      Add any facts about user preferences to long term memory. Always use this if users mention a preference.
      The input to this tool should be a string that describes the user's preference, not the question or answer.
  get_memory:
    _type: get_memory
    memory: saas_memory
    description: |
      Always call this tool before calling any other tools, even if the user does not mention to use it.
      The question should be about user preferences which will help you format your response.
      For example: "How does the user like responses formatted?"

llms:
  nim_llm:
    _type: nim
    model_name: meta/llama-3.3-70b-instruct
    temperature: 0
    max_tokens: 4096
    top_p: 1

embedders:
  milvus_embedder:
    _type: nim
    model_name: nvidia/nv-embedqa-e5-v5
    truncate: "END"

workflow:
  _type: react_agent
  tool_names:
   - cuda_retriever_tool
   - mcp_retriever_tool
   - add_memory
   - get_memory
  verbose: true
  llm_name: nim_llm
```

Notice in the configuration above that the only addition to the configuration that was required to add long term memory to the agent was a `memory` section in the configuration specifying:
- The type of memory to use (`mem0_memory`)
- The name of the memory (`saas_memory`)

Then, we used native AIQ toolkit functions for getting memory and adding memory to the agent. These functions are:
- `add_memory`: This function is used to add any facts about user preferences to long term memory.
- `get_memory`: This function is used to retrieve any facts about user preferences from long term memory.

Each function was given a description that helps the agent know when to use it as a tool. With the configuration in place, we can run the workflow again.
This time, we will tell the agent about how we like our responses formatted, and notice if it stores that fact to long term memory.

```bash
aiq run --config_file=examples/RAG/simple_rag/configs/milvus_memory_rag_config.yml --input "How do I install CUDA? I like responses with a lot of emojis in them! :)"
```

The expected output of the above run is:

```console
$ aiq run --config_file=examples/RAG/simple_rag/configs/milvus_memory_rag_config.yml --input "How do I install CUDA? I like responses with a lot of emojis in them! :)"
2025-04-23 16:56:40,025 - aiq.runtime.loader - WARNING - Loading module 'aiq_automated_description_generation.register' from entry point 'aiq_automated_description_generation' took a long time (478.030443 ms). Ensure all imports are inside your registered functions.
2025-04-23 16:56:40,222 - aiq.runtime.loader - WARNING - Loading module 'aiq_swe_bench.register' from entry point 'aiq_swe_bench' took a long time (103.739262 ms). Ensure all imports are inside your registered functions.
2025-04-23 16:56:40,376 - aiq.cli.commands.start - INFO - Starting AIQ toolkit from config file: 'examples/RAG/simple_rag/configs/milvus_memory_rag_config.yml'
2025-04-23 16:56:40,385 - aiq.cli.commands.start - WARNING - The front end type in the config file (fastapi) does not match the command name (console). Overwriting the config file front end.
2025-04-23 16:56:41,738 - httpx - INFO - HTTP Request: GET https://api.mem0.ai/v1/ping/ "HTTP/1.1 200 OK"
2025-04-23 16:56:41,933 - aiq.retriever.milvus.retriever - INFO - Mivlus Retriever using _search for search.
2025-04-23 16:56:41,946 - aiq.retriever.milvus.retriever - INFO - Mivlus Retriever using _search for search.

Configuration Summary:
--------------------
Workflow Type: react_agent
Number of Functions: 4
Number of LLMs: 1
Number of Embedders: 1
Number of Memory: 1
Number of Retrievers: 2

2025-04-23 16:56:44,973 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Agent input: How do I install CUDA? I like responses with a lot of emojis in them! :)
Agent's thoughts:
Thought: The user is asking about installing CUDA, and they have a preference for responses with a lot of emojis. I should first try to retrieve information about the user's preference for response format.

Action: get_memory
Action Input: {"query": "response format", "top_k": 1, "user_id": "current_user"}

------------------------------
2025-04-23 16:56:45,143 - httpx - INFO - HTTP Request: POST https://api.mem0.ai/v1/memories/search/ "HTTP/1.1 200 OK"
2025-04-23 16:56:45,145 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Calling tools: get_memory
Tool's input: {"query": "response format", "top_k": 1, "user_id": "current_user"}
Tool's response:
Memories as a JSON:
[{"conversation": [], "tags": ["user_preferences"], "metadata": {}, "user_id": "current_user", "memory": "Likes responses with a lot of emojis"}]
------------------------------
2025-04-23 16:56:45,875 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Agent input: How do I install CUDA? I like responses with a lot of emojis in them! :)
Agent's thoughts:
Thought: The user likes responses with a lot of emojis. Now, I should try to find information about installing CUDA.

Action: cuda_retriever_tool
Action Input: {"query": "install CUDA"}
------------------------------
2025-04-23 16:56:46,203 - aiq.tool.retriever - INFO - Retrieved 10 records for query install CUDA.
2025-04-23 16:56:46,230 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Calling tools: cuda_retriever_tool
Tool's input: {"query": "install CUDA"}
Tool's response:
{"results": [{"page_content": "1. Introduction \u2014 Installation Guide Windows 12.8 documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n1.1. System Requirements\n1.2. About This Document\n\n\n2. Installing CUDA Development Tools\n2.1. Verify You Have a CUDA-capable GPU\n2.2. Download the NVIDIA CUDA Toolkit\n2.3. Install the CUDA Software\n2.3.1. Uninstalling the CUDA Software\n\n\n2.4. Using Conda to Install the CUDA Software\n2.4.1. Conda Overview\n2.4.2. Installation\n2.4.3. Uninstallation\n2.4.4. Installing Previous CUDA Releases\n\n\n2.5. Use a Suitable Driver Model\n2.6. Verify the Installation\n2.6.1. Running the Compiled Examples\n\n\n\n\n3. Pip Wheels\n4. Compiling CUDA Programs\n4.1. Compiling Sample Projects\n4.2. Sample Projects\n4.3. Build Customizations for New Projects\n4.4. Build Customizations for Existing Projects\n\n\n5. Additional Considerations\n6. Notices\n6.1. Notice\n6.2. OpenCL\n6.3. Trademarks\n\n\n...
------------------------------
2025-04-23 16:56:51,636 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Agent input: How do I install CUDA? I like responses with a lot of emojis in them! :)
Agent's thoughts:
Thought: The user likes responses with a lot of emojis. Now, I should try to find information about installing CUDA.

The installation instructions for CUDA are as follows:

1. Verify you have a CUDA-capable GPU.
2. Download the NVIDIA CUDA Toolkit from https://developer.nvidia.com/cuda-downloads.
3. Install the NVIDIA CUDA Toolkit.

For Windows:
- Graphical Installation: Execute the CUDA installer and follow the on-screen prompts.
- Silent Installation: Execute the package with the -s flag.

For Linux:
- Package Manager Installation: Install using RPM or Debian packages, which interface with your system's package management system.
- Runfile Installation: Install using the standalone installer, a .run file that is completely self-contained.

After installation, perform the post-installation actions.

Thought: I now know the final answer

Final Answer: üéâüëç To install CUDA, follow these steps: üìù
1. Verify you have a CUDA-capable GPU üñ•Ô∏è.
2. Download the NVIDIA CUDA Toolkit from https://developer.nvidia.com/cuda-downloads üìä.
3. Install the NVIDIA CUDA Toolkit üìà.
For Windows:
- Graphical Installation: Execute the CUDA installer and follow the on-screen prompts üì∫.
- Silent Installation: Execute the package with the -s flag üó£Ô∏è.
For Linux:
- Package Manager Installation: Install using RPM or Debian packages, which interface with your system's package management system üì¶.
- Runfile Installation: Install using the standalone installer, a .run file that is completely self-contained üìÅ.
After installation, perform the post-installation actions üìù. üéâüëç
------------------------------
2025-04-23 16:56:51,642 - aiq.front_ends.console.console_front_end_plugin - INFO -
--------------------------------------------------
Workflow Result:
['üéâüëç To install CUDA, follow these steps: üìù\n1. Verify you have a CUDA-capable GPU üñ•Ô∏è.\n2. Download the NVIDIA CUDA Toolkit from https://developer.nvidia.com/cuda-downloads üìä.\n3. Install the NVIDIA CUDA Toolkit üìà.\nFor Windows: \n- Graphical Installation: Execute the CUDA installer and follow the on-screen prompts üì∫.\n- Silent Installation: Execute the package with the -s flag üó£Ô∏è.\nFor Linux: \n- Package Manager Installation: Install using RPM or Debian packages, which interface with your system's package management system üì¶.\n- Runfile Installation: Install using the standalone installer, a .run file that is completely self-contained üìÅ.\nAfter installation, perform the post-installation actions üìù. üéâüëç']
--------------------------------------------------
```

Notice above that the agent called the `add_memory` tool after retrieving the information about installing CUDA. The `add_memory` tool was given the conversation between the user and the assistant, the tags for the memory, and the metadata for the memory.

Now, we can try another invocation of the agent without mentioning our preference to see if it remembers our preference from the previous conversation.

```bash
aiq run --config_file=examples/RAG/simple_rag/configs/milvus_memory_rag_config.yml --input "How do I install CUDA?"
```

The expected output of the above run is:

```console
$ aiq run --config_file=examples/RAG/simple_rag/configs/milvus_memory_rag_config.yml --input "How do I install CUDA?"
2025-04-23 16:59:21,197 - aiq.runtime.loader - WARNING - Loading module 'aiq_automated_description_generation.register' from entry point 'aiq_automated_description_generation' took a long time (444.273233 ms). Ensure all imports are inside your registered functions.
2025-04-23 16:59:21,517 - aiq.cli.commands.start - INFO - Starting AIQ toolkit from config file: 'examples/RAG/simple_rag/configs/milvus_memory_rag_config.yml'
2025-04-23 16:59:21,525 - aiq.cli.commands.start - WARNING - The front end type in the config file (fastapi) does not match the command name (console). Overwriting the config file front end.
2025-04-23 16:59:22,833 - httpx - INFO - HTTP Request: GET https://api.mem0.ai/v1/ping/ "HTTP/1.1 200 OK"
2025-04-23 16:59:23,029 - aiq.retriever.milvus.retriever - INFO - Mivlus Retriever using _search for search.
2025-04-23 16:59:23,041 - aiq.retriever.milvus.retriever - INFO - Mivlus Retriever using _search for search.

Configuration Summary:
--------------------
Workflow Type: react_agent
Number of Functions: 4
Number of LLMs: 1
Number of Embedders: 1
Number of Memory: 1
Number of Retrievers: 2

2025-04-23 16:59:24,882 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Agent input: How do I install CUDA?
Agent's thoughts:
Thought: To answer the user's question, I need to find information about installing CUDA. I should use the get_memory tool to see if the user has any preferences for how the response should be formatted.

Action: get_memory
Action Input: {"query": "response format preferences", "top_k": 1, "user_id": "current_user"}

------------------------------
2025-04-23 16:59:25,049 - httpx - INFO - HTTP Request: POST https://api.mem0.ai/v1/memories/search/ "HTTP/1.1 200 OK"
2025-04-23 16:59:25,051 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Calling tools: get_memory
Tool's input: {"query": "response format preferences", "top_k": 1, "user_id": "current_user"}
Tool's response:
Memories as a JSON:
[{"conversation": [], "tags": ["user_preferences"], "metadata": {}, "user_id": "current_user", "memory": "Likes responses with a lot of emojis"}]
------------------------------
2025-04-23 16:59:27,888 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Agent input: How do I install CUDA?
Agent's thoughts:
Thought: The user likes responses with a lot of emojis. Now, I need to find information about installing CUDA. I should use the cuda_retriever_tool to find the installation steps.

Action: cuda_retriever_tool
Action Input: {"query": "install CUDA"}
------------------------------
2025-04-23 16:59:28,217 - aiq.tool.retriever - INFO - Retrieved 10 records for query install CUDA.
2025-04-23 16:59:28,221 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Calling tools: cuda_retriever_tool
Tool's input: {"query": "install CUDA"}
Tool's response:
{"results": [{"page_content": "1. Introduction \u2014 Installation Guide Windows 12.8 documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n1.1. System Requirements\n1.2. About This Document\n\n\n2. Installing CUDA Development Tools\n2.1. Verify You Have a CUDA-capable GPU\n2.2. Download the NVIDIA CUDA Toolkit\n2.3. Install the CUDA Software\n2.3.1. Uninstalling the CUDA Software\n\n\n2.4. Using Conda to Install the CUDA Software\n2.4.1. Conda Overview\n2.4.2. Installation\n2.4.3. Uninstallation\n2.4.4. Installing Previous CUDA Releases\n\n\n2.5. Use a Suitable Driver Model\n2.6. Verify the Installation\n2.6.1. Running the Compiled Examples\n\n\n\n\n3. Pip Wheels\n4. Compiling CUDA Programs\n4.1. Compiling Sample Projects\n4.2. Sample Projects\n4.3. Build Customizations for New Projects\n4.4. Build Customizations for Existing Projects\n\n\n5. Additional Considerations\n6. Notices\n6.1. Notice\n6.2. OpenCL\n6.3. Trademarks\n\n\n...
------------------------------
2025-04-23 16:59:36,163 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Agent input: How do I install CUDA?
Agent's thoughts:
Thought: The user likes responses with a lot of emojis. Now, I need to find information about installing CUDA. The results from the cuda_retriever_tool provide detailed steps for installing CUDA on various operating systems, including Windows and Linux.

To install CUDA on Windows, the user should:
1. Verify that their system has a CUDA-capable GPU üñ•Ô∏è
2. Download the NVIDIA CUDA Toolkit from the official website üì¶
3. Install the CUDA Software by executing the CUDA installer and following the on-screen prompts üíª
4. Test that the installed software runs correctly and communicates with the hardware üìä

To install CUDA on Linux, the user should:
1. Perform the pre-installation actions üìù
2. Choose an installation method: local repo or network repo ü§î
3. Install the CUDA Toolkit using the chosen method üì¶
4. Perform the post-installation actions üìà

Here are the detailed steps for each operating system:

**Windows:**
1. Verify that your system has a CUDA-capable GPU üñ•Ô∏è
2. Download the NVIDIA CUDA Toolkit from the official website üì¶
3. Install the CUDA Software by executing the CUDA installer and following the on-screen prompts üíª
4. Test that the installed software runs correctly and communicates with the hardware üìä

**Linux:**
1. Perform the pre-installation actions üìù
2. Choose an installation method: local repo or network repo ü§î
3. Install the CUDA Toolkit using the chosen method üì¶
4. Perform the post-installation actions üìà

For example, on Ubuntu, the user can install CUDA using the following commands:

  sudo dpkg -i cuda-repo-<distro>-X-Y-local_<version>*_<arch>.deb
  sudo cp /var/cuda-repo-<distro>-X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/
  sudo mv cuda-<distro>.pin /etc/apt/preferences.d/cuda-repository-pin-600
  sudo apt-get update
  sudo apt-get install cuda-toolkit

Replace `<distro>`, `<version>`, and `<arch>` with the corresponding values for your system ü§ñ

Final Answer: üéâ To install CUDA, follow these steps:
1. Verify that your system has a CUDA-capable GPU üñ•Ô∏è
2. Download the NVIDIA CUDA Toolkit from the official website üì¶
3. Install the CUDA Software by executing the CUDA installer and following the on-screen prompts üíª
4. Test that the installed software runs correctly and communicates with the hardware üìä
For Linux, choose an installation method and install the CUDA Toolkit using the chosen method üì¶
Remember to perform the post-installation actions üìà
üëç
------------------------------
2025-04-23 16:59:36,167 - aiq.front_ends.console.console_front_end_plugin - INFO -
--------------------------------------------------
Workflow Result:
['üéâ To install CUDA, follow these steps: \n1. Verify that your system has a CUDA-capable GPU üñ•Ô∏è\n2. Download the NVIDIA CUDA Toolkit from the official website üì¶\n3. Install the CUDA Software by executing the CUDA installer and following the on-screen prompts üíª\n4. Test that the installed software runs correctly and communicates with the hardware üìä\nFor Linux, choose an installation method and install the CUDA Toolkit using the chosen method üì¶\nRemember to perform the post-installation actions üìà\nüëç']
--------------------------------------------------
```

We see from the above output that the agent was able to successfully retrieve our preference for emoji's in responses from long term memory and use it to format the response to our question about installing CUDA.

In this way, you can easily construct an agent that answers questions about your knowledge base and stores long term memories, all without any agent code required!

Note: The long-term memory feature relies on LLM-based tool invocation, which can occasionally be non-deterministic. If you notice that the memory functionality isn't working as expected (e.g., the agent doesn't remember your preferences), simply re-run your first and second inputs. This will help ensure the memory tools are properly invoked and your preferences are correctly stored.

## Adding Additional Tools
This workflow can be further enhanced by adding additional tools. Included with this example are two additional tools: `tavily_internet_search` and `code_generation`.

Prior to using the `tavily_internet_search` tool, create an account at [`tavily.com`](https://tavily.com/) and obtain an API key. Once obtained, set the `TAVILY_API_KEY` environment variable to the API key:
```bash
export TAVILY_API_KEY=<YOUR_TAVILY_API_KEY>
```
or update the workflow config file to include the `api_key`.

These workflows demonstrate how agents can use multiple tools in tandem to provide more robust responses. Both `milvus_memory_rag_tools_config.yml` and `milvus_rag_tools_config.yml` use these additional tools.

We can now run one of these workflows with a slightly more complex input.

```bash
aiq run --config_file examples/RAG/simple_rag/configs/milvus_rag_tools_config.yml --input "How do I install CUDA and get started developing with it? Provide example python code"
```
The expected output of the above run is:
````console
$ aiq run --config_file examples/RAG/simple_rag/configs/milvus_rag_tools_config.yml --input "How do I install CUDA and get started developing with it? Provide example python code"
2025-04-23 20:31:34,456 - aiq.runtime.loader - WARNING - Loading module 'aiq_automated_description_generation.register' from entry point 'aiq_automated_description_generation' took a long time (491.573811 ms). Ensure all imports are inside your registered functions.
2025-04-23 20:31:34,779 - aiq.cli.commands.start - INFO - Starting AIQ toolkit from config file: 'examples/RAG/simple_rag/configs/milvus_rag_tools_config.yml'
2025-04-23 20:31:34,788 - aiq.cli.commands.start - WARNING - The front end type in the config file (fastapi) does not match the command name (console). Overwriting the config file front end.
2025-04-23 20:31:34,950 - aiq.retriever.milvus.retriever - INFO - Mivlus Retriever using _search for search.
2025-04-23 20:31:34,960 - aiq.retriever.milvus.retriever - INFO - Mivlus Retriever using _search for search.
2025-04-23 20:31:34,964 - aiq.profiler.utils - WARNING - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function code_generation_tool by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.
2025-04-23 20:31:34,966 - aiq.plugins.langchain.tools.code_generation_tool - INFO - Initializing code generation tool
Getting tool LLM from config
2025-04-23 20:31:34,968 - aiq.plugins.langchain.tools.code_generation_tool - INFO - Filling tool's prompt variable from config
2025-04-23 20:31:34,968 - aiq.plugins.langchain.tools.code_generation_tool - INFO - Initialized code generation tool

Configuration Summary:
--------------------
Workflow Type: react_agent
Number of Functions: 4
Number of LLMs: 1
Number of Embedders: 1
Number of Memory: 0
Number of Retrievers: 2

2025-04-23 20:31:36,778 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Agent input: How do I install CUDA and get started developing with it? Provide example python code
Agent's thoughts:
Thought: To answer this question, I need to provide information on how to install CUDA and get started with developing applications using it. I also need to provide example Python code to demonstrate its usage.

Action: cuda_retriever_tool
Action Input: {"query": "installing CUDA and getting started"}

------------------------------
2025-04-23 20:31:37,097 - aiq.tool.retriever - INFO - Retrieved 10 records for query installing CUDA and getting started.
2025-04-23 20:31:37,099 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Calling tools: cuda_retriever_tool
Tool's input: {"query": "installing CUDA and getting started"}
Tool's response:
{"results": [{"page_content": "1. Introduction \u2014 Installation Guide Windows 12.8 documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n1.1. System Requirements\n1.2. About This Document\n\n\n2. Installing CUDA Development Tools\n2.1. Verify You Have a CUDA-capable GPU\n2.2. Download the NVIDIA CUDA Toolkit\n2.3. Install the CUDA Software\n2.3.1. Uninstalling the CUDA Software\n\n\n2.4. Using Conda to Install the CUDA Software\n2.4.1. Conda Overview\n2.4.2. Installation\n2.4.3. Uninstallation\n2.4.4. Installing Previous CUDA Releases\n\n\n2.5. Use a Suitable Driver Model\n2.6. Verify the Installation\n2.6.1. Running the Compiled Examples\n\n\n\n\n3. Pip Wheels\n4. Compiling CUDA Programs\n4.1. Compiling Sample Projects\n4.2. Sample Projects\n4.3. Build Customizations for New Projects\n4.4. Build Customizations for Existing Projects\n\n\n5. Additional Considerations\n6. Notices\n6.1. Notice\n6.2. OpenCL\n6.3. Trademarks\n\n\n...
------------------------------
2025-04-23 20:31:46,243 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Agent input: How do I install CUDA and get started developing with it? Provide example python code
Agent's thoughts:
Thought: The provided information from the CUDA documentation gives a detailed guide on how to install CUDA on various operating systems, including Windows and Linux. It also provides information on how to verify the installation and run sample programs to test the setup. However, to provide example Python code, I need to use the code generation tool.

Action: code_generation_tool
Action Input: {"query": "Python code example using CUDA"}
------------------------------
2025-04-23 20:31:46,251 - aiq.plugins.langchain.tools.code_generation_tool - INFO - Running code generation tool
2025-04-23 20:31:47,931 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Calling tools: code_generation_tool
Tool's input: {"query": "Python code example using CUDA"}
Tool's response:
```python
import numpy as np
import cupy as cp

# Create a sample array
arr = np.array([1, 2, 3, 4, 5])

# Transfer the array to the GPU
arr_gpu = cp.asarray(arr)

# Perform some operations on the GPU
result_gpu = cp.square(arr_gpu)

# Transfer the result back to the CPU
result_cpu = cp.asnumpy(result_gpu)

print(result_cpu)
```
------------------------------
2025-04-23 20:31:52,241 - aiq.agent.react_agent.agent - INFO -
------------------------------
[AGENT]
Agent input: How do I install CUDA and get started developing with it? Provide example python code
Agent's thoughts:
Thought: I now know the final answer

Final Answer: To install CUDA and get started with developing applications using it, you can follow the steps outlined in the CUDA documentation. This includes verifying that you have a CUDA-capable GPU, downloading the NVIDIA CUDA Toolkit, and installing the CUDA software. After installation, you can verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware by compiling and running sample programs.

Here's an example Python code that demonstrates how to use CUDA:
```python
import numpy as np
import cupy as cp

# Create a sample array
arr = np.array([1, 2, 3, 4, 5])

# Transfer the array to the GPU
arr_gpu = cp.asarray(arr)

# Perform some operations on the GPU
result_gpu = cp.square(arr_gpu)

# Transfer the result back to the CPU
result_cpu = cp.asnumpy(result_gpu)

print(result_cpu)
```
This code creates a sample array, transfers it to the GPU, performs some operations on the GPU, and then transfers the result back to the CPU. The output of this code will be the squared values of the original array.
------------------------------
2025-04-23 20:31:52,244 - aiq.front_ends.console.console_front_end_plugin - INFO -
--------------------------------------------------
Workflow Result:
["To install CUDA and get started with developing applications using it, you can follow the steps outlined in the CUDA documentation. This includes verifying that you have a CUDA-capable GPU, downloading the NVIDIA CUDA Toolkit, and installing the CUDA software. After installation, you can verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware by compiling and running sample programs.\n\nHere's an example Python code that demonstrates how to use CUDA:\n```python\nimport numpy as np\nimport cupy as cp\n\n# Create a sample array\narr = np.array([1, 2, 3, 4, 5])\n\n# Transfer the array to the GPU\narr_gpu = cp.asarray(arr)\n\n# Perform some operations on the GPU\nresult_gpu = cp.square(arr_gpu)\n\n# Transfer the result back to the CPU\nresult_cpu = cp.asnumpy(result_gpu)\n\nprint(result_cpu)\n```\nThis code creates a sample array, transfers it to the GPU, performs some operations on the GPU, and then transfers the result back to the CPU. The output of this code will be the squared values of the original array."]
--------------------------------------------------
````

## Using Inference Time Scaling
You can also use the toolkit's experimental `inference_time_scaling` feature to scale the inference time of the agent. This feature allows you to control the inference time of the agent. Particularly, in this example, we demonstrate how to enable multiple
executions of the retrieval agent with a higher LLM temperature to encourage diversity. We then merge the outputs of the multiple runs with another LLM call to synthesize one comprehensive answer from multiple searches.

An example configuration can be found in the `configs/milvus_rag_config_its.yml` file. Notably, it has a few additions to the standard configuration: 
- An `its_strategies` section of the configuration that details which inference time scaling techniques will be used in the workflow
- A `selection_strategy` called `llm_based_agent_output_merging` selection, that takes the output of multiple workflow runs and combines them using a single LLM call. 
- A new `workflow` entrypoint called the `execute_score_select` function. The function executes the `augmented_fn` (the ReAct agent here) `num_iterations` times, and then passes the outputs to the selector. 

To run this workflow, you can use the following command:
```bash
aiq run --config_file examples/simple_rag/configs/milvus_rag_config_its.yml --input "What is the difference between CUDA and MCP?"
```

You should see output that looks similar to the following: 
```console
.....
Configuration Summary:
--------------------
Workflow Type: execute_score_select_function
Number of Functions: 3
Number of LLMs: 1
Number of Embedders: 1
Number of Memory: 0
Number of Retrievers: 2
Number of ITS Strategies: 1
2025-06-30 13:33:59,629 - aiq.experimental.decorators.experimental_warning_decorator - WARNING - This function is experimental and the API may change in future releases. Function: get_its_strategy
...
...
...
[AGENT]
Agent input: How does CUDA compare to MCP?
Agent's thoughts: 
Thought: Now that I have information about both CUDA and MCP, I can compare them. CUDA is a library developed by NVIDIA for parallel computing, while MCP stands for Model Context Protocol, which is an open protocol that standardizes how applications provide context to LLMs (Large Language Models).

 CUDA and MCP are two different technologies with different purposes. CUDA is a parallel computing platform and programming model developed by NVIDIA, while MCP is an open protocol for providing context to Large Language Models. They are not directly comparable, but they can be used together in certain applications.

Final Answer: CUDA and MCP are two different technologies with different purposes. CUDA is a parallel computing platform and programming model developed by NVIDIA, while MCP is an open protocol for providing context to Large Language Models. They are not directly comparable, but they can be used together in certain applications.
------------------------------
2025-06-30 13:34:15,706 - aiq.agent.react_agent.agent - INFO - 
------------------------------
[AGENT]
Agent input: How does CUDA compare to MCP?
Agent's thoughts: 
Thought: I have been provided with information about CUDA and MCP. CUDA is a library developed by NVIDIA for parallel computing, while MCP stands for Model Context Protocol, which is an open protocol that standardizes how applications provide context to LLMs (Large Language Models). 

To compare CUDA and MCP, we need to understand that they serve different purposes. CUDA is a computing platform and programming model that enables dramatic increases in computing performance by harnessing the power of the GPU. On the other hand, MCP is a protocol that enables LLMs to securely access tools and data sources.

Since CUDA and MCP are fundamentally different, a direct comparison between the two may not be meaningful. However, we can say that CUDA is focused on computing and parallel processing, while MCP is focused on providing a standardized way for LLMs to access tools and data sources.

Final Answer: CUDA and MCP serve different purposes and cannot be directly compared. CUDA is a computing platform and programming model for parallel computing, while MCP is a protocol for providing context to LLMs.
------------------------------
2025-06-30 13:34:18,083 - aiq.agent.react_agent.agent - INFO - 
------------------------------
[AGENT]
Agent input: How does CUDA compare to MCP?
Agent's thoughts: 
Thought: I have been provided with information about CUDA and MCP. CUDA is a library developed by NVIDIA for parallel computing, while MCP stands for Model Context Protocol, which is an open protocol that standardizes how applications provide context to LLMs (Large Language Models). 

To compare CUDA and MCP, we need to consider their purposes and use cases. CUDA is primarily used for parallel computing, allowing developers to harness the power of NVIDIA GPUs to perform complex computations. On the other hand, MCP is designed to provide a standardized way for applications to interact with LLMs, enabling them to access and utilize the capabilities of these models.

Since I have already retrieved information about both CUDA and MCP, I can now provide a comparison between the two.

Final Answer: CUDA and MCP are two different technologies with distinct purposes. CUDA is a library for parallel computing, while MCP is a protocol for providing context to LLMs. While both technologies are used in different fields, they share a common goal of enabling developers to create powerful and efficient applications. CUDA is used for compute-intensive tasks such as scientific simulations, data analytics, and machine learning, whereas MCP is used for natural language processing and other AI-related tasks. In summary, CUDA and MCP are complementary technologies that can be used together to create innovative applications, but they are not directly comparable.
------------------------------
2025-06-30 13:34:18,086 - aiq.experimental.inference_time_scaling.functions.execute_score_select_function - INFO - Beginning selection
2025-06-30 13:34:20,578 - aiq.experimental.inference_time_scaling.selection.llm_based_output_merging_selector - INFO - Merged output: CUDA and MCP are two distinct technologies with different purposes and cannot be directly compared. CUDA is a parallel computing platform and programming model, primarily used for compute-intensive tasks such as scientific simulations, data analytics, and machine learning, whereas MCP is an open protocol designed for providing context to Large Language Models (LLMs), particularly for natural language processing and other AI-related tasks. While they serve different purposes, CUDA and MCP share a common goal of enabling developers to create powerful and efficient applications. They are complementary technologies that can be utilized together in certain applications to achieve innovative outcomes, although their differences in design and functionality set them apart. In essence, CUDA focuses on parallel computing and is developed by NVIDIA, whereas MCP is focused on context provision for LLMs, making them unique in their respective fields but potentially synergistic in specific use cases.
2025-06-30 13:34:20,578 - aiq.front_ends.console.console_front_end_plugin - INFO - 
--------------------------------------------------
Workflow Result:
['CUDA and MCP are two distinct technologies with different purposes and cannot be directly compared. CUDA is a parallel computing platform and programming model, primarily used for compute-intensive tasks such as scientific simulations, data analytics, and machine learning, whereas MCP is an open protocol designed for providing context to Large Language Models (LLMs), particularly for natural language processing and other AI-related tasks. While they serve different purposes, CUDA and MCP share a common goal of enabling developers to create powerful and efficient applications. They are complementary technologies that can be utilized together in certain applications to achieve innovative outcomes, although their differences in design and functionality set them apart. In essence, CUDA focuses on parallel computing and is developed by NVIDIA, whereas MCP is focused on context provision for LLMs, making them unique in their respective fields but potentially synergistic in specific use cases.']
--------------------------------------------------
```

**Note**: The workflow was executed three times, with three different `Final Answer` sections. The selector then combines them into a single, comprehensive response.